---
title: 'Essay 4: KNN and Decision trees'
author: "Dhruvi, Vi, Huy, Harshal"
date: "4/16/2021"
output: html_document
---

## (1) INTRODUCTION

### KNN and Decision trees



### Loading R packages

```{r, results=FALSE, warning=FALSE, message=FALSE}
library("dplyr")
library("readr")
library("stats")
library("GGally")
library("ggpubr")
library("tidyverse") #for data manipulation and visualization
library("class")
library("gmodels")
```

## (2) DATA DESCRIPTION

### Examples of data and problem

**Iris Flower Dataset**

The dataset was taken from the UCI Machine Learning Repository and  includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.

The columns are as follows:

+ SepalLengthCm = Length of the sepal (in cm)
+ SepalWidthCm = Width of the sepal (in cm)
+ PetalLengthCm = Length of the petal (in cm)
+ PetalWidthCm = Width of the petal (in cm)
+ Species = Species name

`The objective is to predict the species of the Iris flower given the sepal and petal characteristics. We will implement the KNN algorithm to interpret those results.`

**Importing the data file**

```{r, echo=TRUE, results='hide'}
iris <- read.csv("Iris.csv", stringsAsFactors = TRUE) %>% 
  select(-"Id") %>% 
  drop_na() %>% #remove NA values
  distinct() # remove duplicated rows 

summary(iris)

set.seed(99) #randomize the dataset to avoid heavily discarding one species during data split
rows <- sample(nrow(iris)) 
iris <- iris[rows, ] # this is the randomized data set with jumbled rows
```

**Normalize the numeric columns**

This feature is of paramount importance since the scale used for the values for each variable might be different. The best practice is to normalize the data and transform all the values to a common scale.

```{r}
normalize <- function(x) {
  return ( (x - min(x)) / (max(x) - min(x)) )
}
```

Once we run this code, we are required to normalize the numeric features in the data set. Instead of normalizing each of the 4 individual variables we use lappy to apply the function to all variables at once:

```{r}
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))
```

We start from the 1st variable (after removing id), SepalLengthCm. We only normalize upto the 4th column since the 5th col, species, is not numeric. The function lapply() applies normalize() to each feature in the data frame. The final result is stored to iris_norm data frame using as.data.frame() function. 

```{r}
# you can check to see if it's been properly normalized
summary(iris_norm)
```

**Create the test and train Data**

The KNN algorithm is applied to the training data set and the results are verified on the test data set.

For this, we divide the data set into 2 portions in the ratio of 103:44 for the training and test data set respectively.

We shall divide the iris_norm data frame into train and test data frames:

```{r}
iris.train<- iris_norm[1:103,]
iris.test<- iris_norm[104:147,]
```

A blank value in each of the above statements indicate that all rows and columns should be included.

Our target variable is ‘Species’ which we have not included in our training and test data sets. 

This following code takes the species in column 5 of the iris_rand data frame and in turn creates iris_train_labels and iris_test_labels data frame.

```{r}
iris_train_labels <- iris[1:103, 5]
iris_test_labels <- iris[104:147, 5]
```

### Visualization

We visualize how the sepal and petal charcateristics differ based on the Iris species in the following graphs.

In the following graph comparing the Sepal characteristics in the different species, 

+ we observe that virginia tends to have a higher length of the other two and setosa has the shortest sepal length.
+ we also observe that setosa tends to have a highest width of the three species while versicolor has the shortest sepal width.

```{r}
A <- iris %>% 
  ggplot() +
  geom_boxplot(aes(x=Species, y=SepalLengthCm, fill=Species), 
               show.legend = FALSE) +
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank())

B <- iris %>% 
  ggplot() +
  geom_boxplot(aes(x=Species, y=SepalWidthCm, fill=Species)) +
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(),
    legend.position = "bottom",
    legend.direction = "horizontal")

ggarrange(A, B, 
          ncol = 2, nrow = 1)
```
In the following graph comparing the Petal charcateristics in the different species, we observe that virginia tends to have the highest Petal length and width of while setosa has the shortest.
We can also determine the range that each characteristic could fall between.

```{r}
C <- iris %>% 
  ggplot() +
  geom_boxplot(aes(x=Species, y=PetalLengthCm, fill=Species), 
               show.legend = FALSE) +
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank())

D <- iris %>% 
  ggplot() +
  geom_boxplot(aes(x=Species, y=PetalWidthCm, fill=Species)) +
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank(), 
    legend.position = "bottom",
    legend.direction = "horizontal")

ggarrange(C, D,
          ncol = 2, nrow = 1)
```
Sepal width and length appear to be correlated but each species is different. Specifically, there’s a lot of overlap between the versicolor and virginica species. The virginica sepal length seems to be a bit longer but otherwise there's quite a lot of overlap.

```{r}
ggplot(data = iris, 
       aes(x = SepalLengthCm, y = SepalWidthCm, color = Species)) + 
  geom_point() + 
  geom_smooth()

ggplot(data = iris, aes(x = SepalLengthCm, y = SepalWidthCm)) + 
  geom_point(size = 3) + 
  geom_smooth(size = 1, se = FALSE) + 
  facet_grid(rows = vars(Species))
```

The petal lengths and widths, however, don't show much correlation. Each species seems one up the other.

```{r}
ggplot(data = iris, aes(x = PetalLengthCm, y = PetalWidthCm)) + 
  geom_point(size = 3) + 
  geom_smooth(size = 1, se = FALSE) + 
  facet_grid(rows = vars(Species))

ggplot(data = iris, 
       aes(x = PetalLengthCm, y = PetalWidthCm, color = Species)) + 
  geom_point() + 
  geom_smooth()
```

## (3) ANALYSIS

### Model Computation

We need to use the knn () function to train a model. It identifies the k-nearest neighbors using Euclidean distance where k is a user-specified number.

**Finding the best k value**

+ k=1: The model is too specific and not generalized well. It also tends to be sensitive to noise. The model accomplishes a high accuracy on train set but will be a poor predictor on new, previously unseen data points. Therefore, we are likely to end up with an overfit model.

+ k=100: The model is too generalized and not a good predictor on both train and test sets. This situation is known as under-fitting.

One of the most commonly used ways to find the optimal K value is to calculate the square root of the total number of observations in the data set. This square root will give you the ‘K’ value. 

So first we find the dimensions of our training data set and then find the square root of that value to get an approx k-value.

```{r}
size <- NROW(iris_train_labels) #size of training data
size
sqrt(size) # the k-value
```

We find that the size of our training data is 103 which gives us a square root of 10.14998. Let's then create 2 KNN models using k=9 and k=11 since using odd values for k is the common practice. This is because KNN predicts through a majority vote and odd k-values help avoid having ties.

```{r}
#library(class)

iris_k9_pred <- knn(
  train = iris.train,
  test = iris.test,
  cl = iris_train_labels,
  k=9
)

iris_k11_pred <- knn(
  train = iris.train, 
  test = iris.test, 
  cl = iris_train_labels,
  k=11
)

iris_k3_pred <- knn(
  train = iris.train,
  test = iris.test,
  cl = iris_train_labels,
  k=3
)

100 * sum(iris_test_labels == iris_k9_pred)/NROW(iris_test_labels)

100 * sum(iris_test_labels == iris_k11_pred)/NROW(iris_test_labels)
```

KNN function takes in as arguments: 

+ the train data which is of dimension 103, 
+ the test data of dim 44, 
+ class which defines the factors of true classifications aka species values of the training set
+ k value = the number of neighbors considered; value determined by the user, and 

returns a factor value of predicted labels for each of the examples in the test data set which is then assigned to the data frames iris_k9_pred, iris_k11_pred. 

### Interpretation of Model: 

Here we instantiate three KNN classifiers with k=9, 11 and 3 respectively. The model takes input the training data, testing data and the labels (species of the flower), and outputs the percentage of successfully predicted data points in the test set.

The KNN model learns on the training data by essentially memorizing the nearest neighbors around each train data point (where the closeness is defined by the Euclidean distance), and evaluates on the testing set by doing a majority vote on values of the neighbors.

Evidently we would observe different predictive behaviors given different sizes of 'k', because a smaller k value tends to make the model more easily influenced by noises in the training data hence over-fitting (result of high variance). But given the well-constructed, relatively simple Iris dataset and a small amount of test data, such differences may not appear very apparent when k values only vary marginally (such as between 11 and 13).

In the next sections, we will calculate and understand the accuracy of our model by comparing the predicted values with original testing data set’s values.

```{r}
KNN_func <- function(train, test, train.label, k){
  return(knn(train, test, train.label, k))
}

accuracy <- function(train, test, train.label, test.label) {
  accuracy_val <- c()
  for (k in 1:30) {
    Knn.pred <- KNN_func(train, test, train.label, k)
    accuracy_val[k] <- 100 * sum(test.label == Knn.pred)/NROW(test.label)
  }

  return(accuracy_val)
}
```

```{r}
k <- 1:30
acc <- accuracy(iris.train, iris.test, iris_train_labels, iris_test_labels)

ggplot(mapping = aes(x=k, y=acc)) +
  geom_point() +
  geom_line()
```

  
## (4) MODEL EVALUATION

### Regression line


### Model Assessment 



### Model Summary



### Coefficients significance



### Prediction and Model accuracy


## (5) CONCLUSION

### Summary



## (6) REFERENCES

 