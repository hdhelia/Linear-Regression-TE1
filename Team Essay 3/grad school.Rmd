---
title: "Logistic Regression"
author: "Dhruvi, Vi, Huy, Harshal"
date: "3/24/2021"
output: html_document
---

## (1) INTRODUCTION

### Generalized Linear Model: Logistic Regression

Logistic regression is used to predict a class, i.e., a probability. Logistic regression can predict a binary outcome accurately.

Imagine you want to predict whether a a woman wants more children or not based on many attributes. The logistic regression is of the form 0/1. y = 0 if she desires more children, y = 1 if she doesn't.

A logistic regression model differs from linear regression model in two ways.

+ First of all, the logistic regression accepts only dichotomous (binary) input as a dependent variable (i.e., a vector of 0 and 1).
+ Secondly, the outcome is measured by the following probabilistic link function called **sigmoid** due to its S-shaped.:

```{r, echo=FALSE}
knitr::include_graphics("sigmoid.jpg")
```

The output of the function is always between 0 and 1. 

```{r, echo=FALSE}
knitr::include_graphics("graph.png")
```

The sigmoid function returns values from 0 to 1. For the classification task, we need a discrete output of 0 or 1.

To convert a continuous flow into discrete value, we can set a decision bound at 0.5. All values above this threshold are classified as 1

```{r, echo=FALSE}
knitr::include_graphics("threshold.jpg")
```

### Loading R packages

```{r, results=FALSE, warning=FALSE, message=FALSE}
library("readxl") #importing stata data file
library("dplyr")
library("stats")
library("GGally")
library("ROCR")
library("ggpubr")
library("tidyverse") #for data manipulation and visualization
library("ggplot2") 
library("tidyr")
```

## (2) DATA DESCRIPTION

### Examples of data and problem

**Admission into Graduate School**

We used a hypothetical data set produced by UCLA to study the effect of certain variables on the likelihood of admission into graduate school. The variables defines are as follows:

+ GRE (Graduate Record Exam scores)
+ GPA (grade point average) 
+ prestige of the undergraduate institution ranks 1 to 4. 1 being most prestigious and 4 being the least.
+ admission into graduate school. The response variable, 0 (don't admit) / 1 (admit), is a binary variable.

`The objective is to predict whether one will get admission into an institution given their grades and the prestige of the institution`.

**Importing the data file**

```{r, echo=TRUE, results='hide'}
admit_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv") 

#convert the numerical values for categorical variables to factors
admit_data$admit <- factor(admit_data$admit,
                           levels=c(0,1),
                           labels=c("No", "Yes"))
admit_data$rank <- factor(admit_data$rank,
                           levels=c(1,2,3,4),
                           labels=c("veryHigh", "high", "medium", "low"))
admit_data
```

**Cleaning and preparing the dataset for logistic regression**

#### Checking for Continuous variables

In the first step, you can see the distribution of the continuous variables.

```{r}
# Use the function select_if() from the dplyr library to 
# select only the numerical columns
continuous <-select_if(admit_data, is.numeric)

# Print the summary statistic
summary(continuous)
```
From the above table, you can see that the data has totally different scales and there's large outliers present in `gre` (i.e. look at the min and first quartile values)

You can deal with it following two steps:

**1. Plot the distribution of count**

```{r}
# Histogram with kernel density curve
ggplot(continuous, aes(x = gre)) +
    geom_density(alpha = .2, fill = "#FF6666")
```

The variable has a few outliers and not well-defined distribution. You can partially tackle this problem by deleting the bottom 3 percent of the GRE scores.

```{r}
# Compute the value of the bottom 3% percent of GRE scores
bottom_percent <- quantile(admit_data$gre, 0.03)
bottom_percent
```

```{r}
# drop the observations above this threshold
admit_drop <-admit_data %>%
filter(gre>bottom_percent)

ggplot(admit_drop, aes(x = gre)) +
    geom_density(alpha = .2, fill = "#FF6666")
```

**2: Standardize the continuous variables**

You can standardize each column to improve the performance because your data does not have the same scale. Mutate the numeric colums and then scale them.

```{r}
admit_rescale <- admit_drop %>%
	mutate_if(is.numeric, funs(as.numeric(scale(.))))

head(admit_rescale)
```

#### Checking factor variables

This step has two objectives:

+ Check the level in each categorical column
+ Define new levels

First, we select the categorical columns.

```{r}
# Store the factor columns in factor in a data frame type.
factor <- data.frame(select_if(admit_rescale, is.factor))
ncol(factor)
```

The dataset contains 2 categorical variables, Admission status and rank of institution.

Next we store the bar chart of each column in a list.

```{r}
# Create graph for each column by automatizing the graphing process
graph <- lapply(names(factor),
    function(x) 
	ggplot(factor, aes(get(x))) +
		geom_bar() +
		theme(axis.text.x = element_text(angle = 90)))

graph # print the 2 graphs we produced
```

### Visualization

No recasting of variables is necessary since we don't have too many levels in either the rank or admission variable. Additionally, despite the fact that some levels of rank have a relatively low number of observations, we find that each level of rank is substantial when it comes to admission.

We can check the number of instances within each group:

```{r}
table(admit_rescale$rank)
```


It is time to check some statistics about our target variables. In the graph below, 

```{r}
ggplot(admit_rescale, aes(x = rank, fill = admit)) +
    geom_bar(position = "fill") +
    theme_classic()
```

```{r}
ggplot(admit_rescale, aes(x = admit, y = gre)) +
    geom_boxplot() +
    stat_summary(fun = mean,
        geom = "point",
        size = 3,
        color = "steelblue") +
    theme_classic()

ggplot(admit_rescale, aes(x = admit, y = gpa)) +
    geom_boxplot() +
    stat_summary(fun = mean,
        geom = "point",
        size = 3,
        color = "steelblue") +
    theme_classic()
```

```{r}
# Plot distribution working time by education
ggplot(admit_rescale, aes(x = gre)) +
    geom_density(aes(color = rank), alpha = 0.5) +
    theme_classic()

ggplot(admit_rescale, aes(x = gpa)) +
    geom_density(aes(color = rank), alpha = 0.5) +
    theme_classic()
```
```{r}
ggplot(admit_rescale, aes(x = gpa, y = gre)) +
    geom_point(aes(color = admit),
        size = 0.5) +
    stat_smooth(method = 'lm',
        formula = y~poly(x, 2),
        se = TRUE,
        aes(color = admit)) +
    theme_classic()
```

```{r}
ggplot(admit_rescale, aes(x = gpa, y = gre)) +
    geom_point(aes(color = rank),
        size = 0.5) +
    stat_smooth(method = 'lm',
        formula = y~poly(x, 2),
        se = TRUE,
        aes(color = rank)) +
    theme_classic()
```

**Correlation**

The next check is to visualize the correlation between the variables. You convert the factor level type to numeric so that you can plot a heat map containing the coefficient of correlation computed with the Spearman method.

```{r}
# Convert data to numeric
corr <- data.frame(lapply(admit_rescale, as.integer))
# plot the heat map with the following arguments:
ggcorr(corr, 
    method = c("pairwise", "spearman"), # compute the correlation
    nbreaks = 6, # Number of break
    hjust = 0.8, # Control position of the variable name in the plot
    label = TRUE, # Add labels in the center of the windows
    label_size = 3, # Size labels
    color = "grey50") # Color of the label
```

### Train and test set

Any supervised machine learning task require to split the data between a train set and a test set. You can use the "function" you created in the other supervised learning tutorials to create a train/test set.

```{r}
set.seed(1234)
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
data_train <- create_train_test(admit_rescale, 0.8, train = TRUE)
data_test <- create_train_test(admit_rescale, 0.8, train = FALSE)

dim(data_train)
dim(data_test)
```


## (3) ANALYSIS

### Model Creation and Computation
This multiple linear regression model tries to find the best line to predict Life Expectancy based on **GRE**, **GPA**,**Rankhigh**,**RankMedium**, and **Ranklow**.

The glm() function is used to determine the beta coefficients for the linear model:

```{r}
formula <- admit~.
logit <- glm(formula, data = data_train, family = 'binomial')
summary(logit)

```
This result shows the intercept (b0) and the beta coefficients (b1, b2, b3, b4 ,b5) for the **GRE**, **GPA**,**Rankhigh**,**RankMedium**, and **Ranklow** variables.
### Interpretation of Model: 

+ Model: `Admit = 0.1683 + 0.2626 * GRE + 0.3642 * GPA + (-0.7197) * Rankhigh + (-1.5836) * RankMedium + (-1.5125) * Ranklow` 

The first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary. This is done to ensure that the predictor variables are significant in relation to Life Expectancy; in this case, they are, so all three of our initial predictor variables will stay in the model. See the section on t-statistic and p-values for a detailed explanation of this.

The intercept b0 = 0.1683 represents the predicted value if all variables equal zero; if **GRE**, **GPA**,**Rankhigh**,**RankMedium**, and **Ranklow** are all zero, the model predicts that the admission rate is 0.005567 on average. 

For a given predictor variable, the coefficient (b) can be interpreted as the average effect on y of a one unit
increase in predictor, holding all other predictors fixed.

For example:
+ssss
+ssss

### Estimates
We find the estimated Admission Rate in percentage given by our model for the respective **GRE**, **GPA**,**Rankhigh**,**RankMedium**, and **Ranklow** values and compare them to the actual values.

**Admission Rate Data**

## (4) MODEL EVALUATION

### Regression line


```{r}
summary(logit)
logit$coef
b0 <- logit$coef[1] # intercept
gre <- logit$coef[2]
gpa <- -logit$coef[3]
rankhigh <- logit$coef[4]
rankmedium <- logit$coef[5]
ranklow <- logit$coef[6]


gre_range <- seq(from=min(data_train$gre), to=max(data_train$gre), by=.01)
gpa_val <- mean(data_train$gpa)

a_logits <- b0 + 
  gre*gre_range + 
  gpa*gpa_val + 
  rankhigh*0 + 
  rankmedium*0 + 
  ranklow * 0


b_logits <- b0 + 
  gre*gre_range + 
  gpa*gpa_val + 
  rankhigh*1 + 
  rankmedium*0 + 
  ranklow * 0

c_logits <- b0 + 
  gre*gre_range + 
  gpa*gpa_val + 
  rankhigh*0 + 
  rankmedium*1 + 
  ranklow * 0

d_logits <- b0 + 
  gre*gre_range + 
  gpa*gpa_val + 
  rankhigh*0 + 
  rankmedium*0 + 
  ranklow *1

# Compute the probibilities (this is what will actually get plotted):
a_probs <- exp(a_logits)/(1 + exp(a_logits))
b_probs <- exp(b_logits)/(1 + exp(b_logits))
c_probs <- exp(c_logits)/(1 + exp(c_logits))
d_probs <- exp(d_logits)/(1 + exp(d_logits))
  
  
plot.data <- data.frame(a=a_probs, b=b_probs, c=c_probs, d=d_probs, X1=gre_range)
plot.data <- gather(plot.data, key=group, value=prob, a:d)
head(plot.data)  

ggplot(plot.data, aes(x=X1, y=prob, color=group)) + # asking it to set the color by the variable "group" is what makes it draw three different lines
  geom_line(lwd=2) + 
  labs(x="X1", y="P(Admission rate)", title="Probability of getting accepted") 

```


### Model Assessment 



### Model Summary
We start by displaying the statistical summary of the model using the R function summary():


```{r}
summary(logit)

```
The summary of our model reveals interesting information. The performance of a logistic regression is evaluated with specific key metrics.

+ AIC (Akaike Information Criteria): This is the equivalent of R2 in logistic regression. It measures the fit when a penalty is applied to the number of parameters. Smaller AIC values indicate the model is closer to the truth.

+ Null deviance: Fits the model only with the intercept. The degree of freedom is n-1. We can interpret it as a Chi-square value (fitted value different from the actual value hypothesis testing).

+ Residual Deviance: Model with all the variables. It is also interpreted as a Chi-square hypothesis testing.

+ Number of Fisher Scoring iterations: Number of iterations before converging.

The output of the glm() function is stored in a list. The code below shows all the items available in the logit variable we constructed to evaluate the logistic regression.
ch
```{r}

lapply(logit, class)[1:3]

logit$coefficients


```



### Coefficients significance
The coefficients table, in the model statistical summary, shows:
+ the estimates of the beta coefficients
+ the standard errors (SE), which defines the accuracy of beta coefficients. For a given beta coefficient,
the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the
confidence intervals and the t-statistic.
+ The z-value is the regression coefficient divided by standard error.

```{r}
summary(logit)$coef
```


### Prediction and Model accuracy


**1. Confusion Matrix **
The confusion matrix is a better choice to evaluate the classification performance compared with the different metrics you saw before. The general idea is to count the number of times True instances are classified are False.


To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets.

```{r}
predict <- predict(logit, data_test, type = 'response')
# confusion matrix
table_mat <- table(data_test$admit, predict > 0.5)
table_mat
```
Each row in a confusion matrix represents an actual target, while each column represents a predicted target. The first row of this matrix considers the income lower than 50k (the False class): 6241 were correctly classified as individuals with income lower than 50k (True negative), while the remaining one was wrongly classified as above 50k (False positive). The second row considers the income above 50k, the positive class were 1229 (True positive), while the True negative was 1074.

You can calculate the model accuracy by summing the true positive + true negative over the total observation

```{r}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
```

The model appears to suffer from one problem, it overestimates the number of false negatives. This is called the accuracy test paradox. We stated that the accuracy is the ratio of correct predictions to the total number of cases. We can have relatively high accuracy but a useless model. It happens when there is a dominant class. If you look back at the confusion matrix, you can see most of the cases are classified as true negative. Imagine now, the model classified all the classes as negative (i.e. lower than 50k). You would have an accuracy of 75 percent (6718/6718+2257). Your model performs better but struggles to distinguish the true positive with the true negative.


**2. Precision vs Recall **
Precision looks at the accuracy of the positive prediction. Recall is the ratio of positive instances that are correctly detected by the classifier;

You can construct two functions to compute these two metrics

```{r}
precision <- function(matrix) {
	# True positive
    tp <- matrix[2, 2]
	# false positive
    fp <- matrix[1, 2]
    return (tp / (tp + fp))
}

recall <- function(matrix) {
# true positive
    tp <- matrix[2, 2]# false positive
    fn <- matrix[2, 1]
    return (tp / (tp + fn))
}


prec <- precision(table_mat)
prec
rec <- recall(table_mat)
rec
```

When the model says it is an individual above 50k, it is correct in only 54 percent of the case, and can claim individuals above 50k in 72 percent of the case.

You can create the  score based on the precision and recall. The  is a harmonic mean of these two metrics, meaning it gives more weight to the lower values.

```{r}
f1 <- 2 * ((prec * rec) / (prec + rec))
f1
```
**3.Precision vs Recall **
It is impossible to have both a high precision and high recall.

If we increase the precision, the correct individual will be better predicted, but we would miss lots of them (lower recall). In some situation, we prefer higher precision than recall. There is a concave relationship between precision and recall.

+ Imagine, you need to predict if a patient has a disease. You want to be as precise as possible.
+ If you need to detect potential fraudulent people in the street through facial recognition, it would be better to catch many people labeled as fraudulent even though the precision is low. The police will be able to release the non-fraudulent individual.

**4.The ROC curve **
The **Receiver Operating Characteristic** curve is another common tool used with binary classification. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve shows the true positive rate (i.e., recall) against the false positive rate. The false positive rate is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate. The true negative rate is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1-specificity

To plot the ROC curve, we need to install a library called RORC. We can find in the conda library. You can type the code:

conda install -c r r-rocr --yes

We can plot the ROC with the prediction() and performance() functions.
```{r}
library(ROCR)
ROCRpred <- prediction(predict, data_test$admit)
ROCRperf <- performance(ROCRpred, 'tpr', 'fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2, 1.7))

```

## (5) CONCLUSION

### Summary



## (6) REFERENCES

“GLM in R: Generalized Linear Model with Example.” Guru99, 2021, www.guru99.com/r-generalized-linear-model.html. 

“Logit Regression” IDRE Stats, stats.idre.ucla.edu/r/dae/logit-regression/
