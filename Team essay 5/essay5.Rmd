---
title: 'Essay 5: Principal Component Analysis and Clustering'
author: "Dhruvi, Vi, Huy, Harshal"
date: "4/23/2021"
output: html_document
---

## (1) INTRODUCTION

### Principal Component Analysis and K-means Clustering


### Loading R packages

```{r, results=FALSE, warning=FALSE, message=FALSE}
library("dplyr")
library("tidyverse") #for data manipulation and visualization
library("corrplot")
library("animation")
library("cluster")
library("GGally")
library("factoextra")
library("ggcorrplot")
```

## (2) DATA DESCRIPTION

### Examples of data and problem

**Wine Data set**

These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The dataset contains 13 attributes with 178 rows.

1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10) Color intensity
11) Hue
12) OD280/OD315 of diluted wines
13) Proline

*Our goal is to try to group similar observations together and determine the number of possible clusters.*

**Importing the data file**

```{r, echo=TRUE, results='hide', warning=FALSE, message=FALSE}

wine.cols = c("Alcohol", "Malic_Acid", "Ash", "Ash_Alcanity", "magnesium", "Total_Phenols", "Flavanoids",	"Nonflavanoid_Phenols", "Proanthocyanins", "Color_Intensity", "Hue", "OD280", "Proline")

wine <- read.table("wine.data", sep = ",", row.names=NULL, col.names=wine.cols, nrows=178)[ ,2:14]

wine <- wine %>% 
  drop_na() %>% #remove NA values
  distinct() # remove duplicated rows

summary(wine)
```

### Visualization

First we plot the histogram of each attribute. The idea here is to visualize the data distribution for each feature.

```{r}
# Histogram for each Attribute
wine %>%
  gather(Attributes, value, 1:13) %>%
  ggplot(aes(x=value, fill=Attributes)) +
  geom_histogram(colour="black", show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free_x") +
  labs(x="Values", y="Frequency",
       title="Wines Attributes - Histograms") +
  theme_bw()
```

```{r}
wine %>%
  gather(Attributes, values) %>%
  ggplot(aes(x=reorder(Attributes, values, FUN=median), y=values, fill=Attributes)) +
  geom_boxplot(show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free", nrow = 4) +
  labs(title="Wines Attributes - Boxplots") +
  theme_bw() +
  theme(axis.title.y=element_blank(),
        axis.title.x=element_blank()) 
```

```{r}
ggcorr(wine,
    nbreaks = 10,
    label = TRUE,
    label_size = 3,
    color = "grey50")
```


**Data Normalization**

We need to normalize the data to express it in the same range of values.

```{r}
winesNorm <- as.data.frame(scale(wine))

summary(winesNorm)
```

## (3) ANALYSIS

### Computation

**K-Means Clustering**

K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity). In k-means clustering, each cluster is represented by its center which corresponds to the mean of points assigned to the cluster.

**The Basic Idea**

The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:
\[
W(C_k) = \sum_{x_i \in C_k}(x_i - \mu_k)^2 \tag{6}
\]

where:
+ $x_i$ is a data point belonging to the cluster $C_k$
+ $\mu_k$ is the mean value of the points assigned to the cluster $C_k$

Each observation ($xi$) is assigned to a given cluster such that the sum of squares (SS) distance of the observation to their assigned cluster centers ($\mu_k$) is minimized.

We define the total within-cluster variation as follows:
\[
tot.withiness = \sum^k_{k=1}W(C_k) = \sum^k_{k=1}\sum_{x_i \in C_k}(x_i - \mu_k)^2 \tag{7}
\]
The total within-cluster sum of square measures the compactness of the clustering and we want it to be as small as possible.

**K-means Algorithm**

The first step when using k-means clustering is to indicate the number of clusters (k) that will be generated in the final solution. The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also known as cluster means or centroids. Next, each of the remaining objects is assigned to it's closest centroid, where closest is defined using the Euclidean distance between the object and the cluster mean. This step is called “cluster assignment step”. After the assignment step, the algorithm computes the new mean value of each cluster. The term cluster “centroid update” is used to design this step. Now that the centers have been recalculated, every observation is checked again to see if it might be closer to a different cluster. All the objects are reassigned again using the updated cluster means. The cluster assignment and centroid update steps are iteratively repeated until the cluster assignments stop changing (i.e until convergence is achieved). That is, the clusters formed in the current iteration are the same as those obtained in the previous iteration.

K-means algorithm can be summarized as follows:

1. Specify the number of clusters (K) to be created (by the analyst)
2. Select randomly k objects from the data set as the initial cluster centers or means
3. Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
4. For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a Kth cluster is a vector of length p containing the means of all variables for the observations in the kth cluster; p is the number of variables.
5. Iteratively minimize the total within sum of square. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations.

**Computing k-means clustering in R**

We can compute k-means in R with the *kmeans* function. Here will group the data into two clusters (*centers = 2*). The *kmeans* function also has an *nstart* option that attempts multiple initial configurations and reports on the best one. For example, adding *nstart = 25* will generate 25 initial configurations. This approach is often recommended.

```{r}
k2 <- kmeans(winesNorm, centers = 2, nstart = 25)
str(k2)
```

The output of *kmeans* is a list with several bits of information. The most important being:

+ _cluster_: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.
+ _centers_: A matrix of cluster centers.
+ _totss_: The total sum of squares.
+ _withinss_: Vector of within-cluster sum of squares, one component per cluster.
+ _tot.withinss_: Total within-cluster sum of squares, i.e. sum(withinss).
+ _betweenss_: The between-cluster sum of squares, i.e. $totss-tot.withinss$.
+ _size_: The number of points in each cluster.

If we print the results we'll see that our groupings resulted in 2 cluster sizes of 54 and 123. We see the cluster centers (means) for the two groups across the thirteen variables . We also get the cluster assignment for each observation.

```{r}
k2
```

### Interpretation of Model: 

We can view our results by using *fviz_cluster*. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) *fviz_cluster* will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

```{r}

fviz_cluster(k2,data=winesNorm)
```

Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure:

```{r}
k3 <- kmeans(winesNorm, centers = 3, nstart = 25)
k4 <- kmeans(winesNorm, centers = 4, nstart = 25)
k5 <- kmeans(winesNorm, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point",  data = winesNorm) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = winesNorm) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = winesNorm) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = winesNorm) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)


```

This visual assessment tells us where true delineations occur between clusters; however, it does not tell us what the optimal number of clusters is.

  
## (4) MODEL EVALUATION

### Regression line


### Model Assessment 



### Model Summary



### Coefficients significance



### Prediction and Model accuracy


## (5) CONCLUSION

### Summary



## (6) REFERENCES

https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/USArrests.html

https://www.kaggle.com/xvivancos/tutorial-clustering-wines-with-k-means

https://www.kaggle.com/rodrigofragoso/explained-k-means-pca-visualization

https://uc-r.github.io/kmeans_clustering

https://www.guru99.com/r-k-means-clustering.html




```{r}
distance <- get_dist(winesNorm) 
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) + 
  theme(
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )
```

```{r}
k2 <- kmeans(winesNorm, centers = 3, nstart = 25)

k2

fviz_cluster(k2, data = winesNorm)
```

```{r}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(winesNorm, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```

```{r}
set.seed(123)

fviz_nbclust(winesNorm, kmeans, method = "wss")
```