---
title: "Essay 2: Multiple Linear Regression"
author: "Dhruvi, Vi, Huy, Jesse, Harshal"
date: "3/15/2021"
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---
## (1) INTRODUCTION

### Formula and basics

Multiple linear regression: `y = b0 + b1*x1 + b2*x2 + b3*x3 + e` where:

+ *x1*, *x2*, *x3* are explanatory variables.
+ **b0**, **b1**, **b2** and **b3** are known as the regression *beta* coeefficients or *parameters*:
  + *b0* is the intercept of regression line: predicted value when *x = 0*.
  + *b1*, *b2*, *b3* are the slope coefficients for each explanatory variables.
+ *e* is the error term, the part of y that can be explained through the regression model.

### Loading R packages

```{r, results=FALSE, warning=FALSE, message=FALSE}
library("readr") #importing csv file/data sets
library("tidyverse") #for data manipulation and visualization
library("broom")
library("modelr")
library("dplyr")
```

## (2) DATA DESCRIPTION

### Examples of data and problem

**Name of the Data-set: Life Expectancy (WHO)**

The data provides information on various factors (immunization, economic, mortality, social, and other health related factors) and their effect on the Life Expectancy from years 2000 to 2015.
The data is collected from 193 countries from WHO and UN website. 

Our analysis utilizes the information on the following variables from the Dataset:

+ Life Expectancy in age
+ Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)
+ Human Development Index in terms of income composition of resources (index ranging from 0 to 1)
+ Number of years of Schooling (years)

*We will study the effects of adult mortality rate, Income Composition of Resources (How productively resources are utilized in a country), and Schooling (years of schooling) on the life expectancy of an individual.*

**Importing the data file**

First download the data file from *Kaggle* and then import and extract out the columns of interest. 
Next, since the dataset has some NA values, we omit the rows containing NA values.

```{r, echo=TRUE, results='hide'}
WHO_data <- read.csv("LifeExpectancyData.csv")

WHO_data <- WHO_data[ , c("Country", "Life.expectancy", "Adult.Mortality", "Income.composition.of.resources", "Schooling")] %>% 
  rename(
    "ICR" = "Income.composition.of.resources"
    )

# omit the rows containing any NA values.
WHO_data <- na.omit(WHO_data)

#randomize the rows so the model isn't specific to a few countries since we create a training and testing subset.
set.seed(42)
rows <- sample(nrow(WHO_data)) 
rand_WHO_data <- WHO_data[rows, ] # this is the randomized data set with jumbled rows

Life_Exp <- rand_WHO_data[1:2600, ] # Training data
Life_Exp_test <- rand_WHO_data[2601:2768, ] # test data

#rand_WHO_data
#Life_Exp
#Life_Exp_test
```

### Visualization

+ Create scatter plots displaying the relationship between the outcome variable (Life expectancy) and each predictor variable:
  - Life expectancy (y) vs. Adult Mortality
  - Life expectancy (y) vs. Income composition of Resources
  - Life expectancy (y) vs. Schooling

+ Add a smoothed line, using stat_smooth()

```{r}
ggplot(Life_Exp, aes(x = Adult.Mortality, y = Life.expectancy)) +
  geom_point() +
  stat_smooth() 

ggplot(Life_Exp, aes(x = ICR, y = Life.expectancy)) +
  geom_point() +
  stat_smooth() 

ggplot(Life_Exp, aes(x = Schooling, y = Life.expectancy)) +
  geom_point() +
  stat_smooth()
```

The graphs above suggest a 
- linearly decreasing relationship between the **Life expectancy** and **Adult Mortality** variables. 
- linearly increasing relationship between the **Life expectancy** and **ICR** variables. 
- linearly increasing relationship between the **Life expectancy** and **Schooling** variables. 

This is a good thing because one important assumption of the linear regression is that the relationship beween the outcome (Rate) and predictor variables (Year) is linear and additive.

It's also possible to compute the correlation between the two variables using the R function **`cor()`**:

```{r}
data.frame(
  'LE_AM' = cor(Life_Exp$Life.expectancy, Life_Exp$Adult.Mortality), 
  'LE_ICR' = cor(Life_Exp$Life.expectancy, Life_Exp$ICR),
  'LE_School' = cor(Life_Exp$Life.expectancy, Life_Exp$Schooling)
  )
```

The correlation coefficient measures the level of the association between two variables x and y. Its value ranges between -1 (perfect negative correlation: when x increases, y decreases) and +1 (perfect positive correlation: when x increases, y increases). A value closer to 0 suggests a weak relationship between the variables. 

*In our data the correlation value between the Adult mortality, ICR, Schooling and Life expectancy is -0.6901876, 0.7248019, and 0.7526172 respectively which are closer to either -1 (adult mortality) and 1 (ICR, schooling) and suggests a decent correlation between our variables i.e the variation of the outcome (Life Expectancy) is explained to a good extent by the predictors.*

Hence, we can continue building a linear model of y as a function of x1, x2, x3 (the 3 predictors).

## (3) ANALYSIS

### Computation

```{r}
model <- lm(Life.expectancy ~ Adult.Mortality + ICR + Schooling, data = Life_Exp)
model
```


### Interpretation of Model

+ Model: `Life.expectancy =  55.21856 + (-0.03102) * Adult.Mortality + (10.45743) * ICR + 1.05254 * Schooling`

### Estimates

We find the estimated Life Expectancy in years (age) given by our model for the respective adult mortality, ICR, schooling values and compare them to the actual values.

+ Model: `Life.expectancy =  55.21856 + (-0.03102) * Adult.Mortality + (10.45743) * ICR + 1.05254 * Schooling`

**Life Expectancy Data**

```{r}
# calculate the estimated life expectancy for the test data based on our model. Created a new column that shoes the model calculated 
# expectancy and also another to show the % difference between the expected values and the actual values.

Life_Exp_test  %>% 
  select(-("Country")) %>% 
  mutate("Expected.LifeExp" = 55.21856 + (-0.03102) * Adult.Mortality + (10.45743) * ICR + 1.05254 * Schooling) %>% 
  mutate("% difference" = ( (Expected.LifeExp - Life.expectancy)/Life.expectancy  ) * 100)
```
  
## (4) MODEL EVALUATION

### Regression line

In the scatter plots below, we plot the fitted regression line for each of the explanatory variables (Adult Mortality, ICR, Schooling) against the response variable (Life Expectancy). The fitted regression lines are surrounded by confidence intervals around it that reflects the uncertainty around the line. 

```{r}
ggplot(Life_Exp, aes(x = Adult.Mortality, y = Life.expectancy)) +
  geom_point() +
  stat_smooth(method = lm) 

ggplot(Life_Exp, aes(x = ICR, y = Life.expectancy)) +
  geom_point() +
  stat_smooth(method = lm) 

ggplot(Life_Exp, aes(x = Schooling, y = Life.expectancy)) +
  geom_point() +
  stat_smooth(method = lm) 
```

### Model Assessment 

In the previous sections, we came up with a linear regression model of Life Expectancy as a function of Adult Mortality, ICR and Schooling : 
`Life.expectancy =  55.21856 + (-0.03102) * Adult.Mortality + (10.45743) * ICR + 1.05254 * Schooling`

In our model, most of the data points fit our model, however, we also see few outliers that are present in our data set.

We see that there is a negative correlation between the explanatory variable (Adult Mortality) and response variable (Life Expectancy). On the contrary, we find positive correlation for both other explanatory variables (ICR, Schooling) with respect to the response variable (Life Expectancy). 

In the following sections, we will determine the quality of our linear model and determine whether the relationship between the explanatory variables and the response variable is significant. Along with that we will also check if the model we have built fits the data that we have used or not. 

### Model Summary

We start by displaying the statistical summary of the model using the R function `summary()`:

```{r}
summary(model)
```

The summary outputs shows 6 components, including:

+ **Call.** Shows the function call used to compute the regression model. 
  + In this case, displaying rate on the y-axis and the predictor variables adult mortality, ICR and schooling on x-axis.
+ **Residuals.** Provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value. 
  + The minimum, median, and maximum are displayed as -24.1441, 0.2752, and 23.9416  respectively.
+ **Coefficients.** Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.
  + The above table shows that all three variables have a statistical impact on life expectancy. It also proves that there is a strong positive relationship between Life Expectancy and ICR and positive and negative relationship with Schooling and Adult Mortality respectively.
+ **Residual standard error** (RSE), **R-squared** (R2) and the **F-statistic** are metrics that are used to check how well the model fits to our data.
  + The RSE is 4.767 with 2596 degrees of freedom, R2 is 0.7377, R2-adjusted is 0.7374 and the F-statistic is 2434 on 3 and 2596 degrees of freedom. 
  + If the p value is lower than 0.05, it indicates the variable is statistically significant. Our p-value for the F-statistic is < 2.2e-16 which is way smaller than 0.05 and indicates a highly significant relationship between, at least, one of the predictor variables and the outcome variable.
  + Adjusted R-squared: Variance explained by the model. In your model, the model explained 74 percent of the variance of y. R squared is always between 0 and 1. The higher the better.

### Coefficients significance

The coefficients table, in the model statistical summary, shows:

+ the estimates of the **beta coefficients**
+ the **standard errors** (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.
+ the **t-statistic** and the associated **p-value**, which defines the statistical significance of the beta coefficients.


```{r}
summary(model)$coefficient
```
**t-statistic and p-values**

For a given predictor, the t-statistic (and its associated p-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.

The statistical hypotheses are as follow:

+ Null hypothesis (H0): the coefficients are equal to zero (i.e., no relationship between x and y)
+ Alternative Hypothesis (Ha): the coefficients are not equal to zero (i.e., there is some relationship between x and y)

Mathematically, for a given beta coefficient (b), the t-test is computed as **`t = (b - 0)/SE(b)`**, where SE(b) is the standard error of the coefficient b. The t-statistic measures the number of standard deviations that b is away from 0. Thus a large t-statistic will produce a small p-value.

The higher the t-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right visually specifies the level of significance. The line below the table shows the definition of these symbols; one star means 0.01 < p < 0.05. The more the stars beside the variable’s p-value, the more significant the variable.

A statistically significant coefficient indicates that there is an association between the predictor (x) and the outcome (y) variable.

*In our case, the p-values for the intercept and the 3 predictors variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables.*

Note: High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped (P. Bruce and Bruce 2017).

**Standard errors and confidence intervals**

The standard error measures the variability/accuracy of the beta coefficients. it can be used to compute the confidence intervals of the coefficients. 

For example, the 95% confidence interval for the coefficient b3 is defined as **`b3 +/- 2*SE(b3)`**, where:

+ the lower limits of `b3 = b3 - 2*SE(b3)` **=** `-0.031 - 2*0.0009` **=** `-0.0328`
+ the upper limits of `b3 = b3 + 2*SE(b3)` **=** `-0.031 + 2*0.0009` **=** `-0.0292`

That is, there is approximately a 95% chance that the interval [-0.0328, -0.0292] will contain the true value of b3. Similarly the 95% confidence interval for b2 can be computed as **`b2 +/- 2*SE(b2)`**.

+ the lower limits of `b2 = b2 - 2*SE(b2)` **=** `10.457 - 2*0.746` **=** `8.995`
+ the upper limits of `b2 = b2 + 2*SE(b2)` **=** `273.416 + 2*0.746` **=** `11.920`

That is, there is approximately a 95% chance that the interval [8.995, 11.920] will contain the true value of b2. Similarly the 95% confidence interval for b1 can be computed as **`b1 +/- 2*SE(b1)`**.

+ the lower limits of `b1 = b1 - 2*SE(b1)` **=** `1.053 - 2*0.047` **=** `0.959`
+ the upper limits of `b1 = b1 + 2*SE(b1)` **=** `1.053 + 2*0.047` **=** `1.145`

That is, there is approximately a 95% chance that the interval [0.959, 1.145] will contain the true value of b1. Similarly the 95% confidence interval for b0 can be computed as **`b0 +/- 2*SE(b0)`**.


+ the lower limits of `b1 = b0 - 2*SE(b0)` **=** `55.219 - 2*0.475` **=** `54.289`
+ the upper limits of `b1 = b0 + 2*SE(b0)` **=** `55.219 + 2*0.475` **=** `56.169`

There is approximately a 95% chance that the interval [54.289, 56.169] will contain the true value of b0.

To get these information, simply type:


```{r}
confint(model)
```

### Model accuracy

The overall quality of the linear regression fit can be assessed using the following three quantities, displayed in the model summary:

1. The Residual Standard Error (RSE).
2. The R-squared (R2)
3. F-statistic


```{r, echo=FALSE}
data.frame('rse' = 4.767, 'r.squared' = 0.7377, 'f.statistic' = 2434, 'p.value' = '< 2.2e-16')
```

**1. The Residual Standard Error (RSE)**

The RSE (also known as the model sigma) is the residual variation, representing the average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors.

RSE provides an absolute measure of patterns in the data that can’t be explained by the model.

Dividing the RSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.

In our example, RSE = 4.767, meaning that the observed life expectancy (per 100k population) deviates from the true regression line by approximately 4.767 units in average.

Whether or not an RSE of 4.767 units is an acceptable prediction error is subjective and depends on the problem context. However, we can calculate the percentage error. In our data set, the mean value of life expectancy is 69.32612, and so the percentage error is 4.767/69.32612 = 6.875%.


```{r}
# RSE calculation

mean(Life_Exp$Life.expectancy)

sigma(model)*100/mean(Life_Exp$Life.expectancy)
```

**2. R-squared and adjusted R-squared**

The R-squared (R2) ranges from 0 to 1 and represents the proportion of information (i.e. variation) in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom.

The R2 measures, how well the model fits the data.

A high value of R2 is a good indication. However, as the value of R2 tends to increase when more predictors are added in the model, such as in multiple linear regression model, you should mainly consider the adjusted R-squared, which is a penalized R2 for a higher number of predictors.

+ An (adjusted) R2 that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.
+ A number near 0 indicates that the regression model did not explain much of the variability in the outcome.

*In our model the R2 value is 0.7377 and the Adjusted R2 value is 0.7374 which are both closer to one and considerably high, meaning that a lot of the variability in the outcome has been explained by our model.*

**3. F-statistic**

The F-statistic gives the overall significance of the model. It assess whether at least one predictor variable has a non-zero coefficient.

As this is not a simple linear regression, this test will provide a significant information about the model that we have calculated to the overall mean of data. We calculate F-statistic with the formula: F-statistic = (MSr/k)/(MSe/(n-k-1)) = (MSr/3)/(MSe/(2596)) = 2434.

A large F-statistic will corresponds to a statistically significant p-value (p < 0.05). 

*In our model, the F-statistic equals 2434 producing a p-value of 2.2e-16. This means that, at least, one of the predictor variables is significantly related to the outcome variable.*

## (5) CONCLUSION

### Summary

```{r, echo=false}
#plot(model)
knitr::include_graphics("qqplot.png")
```


## (6) REFERENCES

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.

Kassambra. 2018. Regression Analysis: Multiple Linear Regression in R. http://www.sthda.com/english/articles/40-regression-analysis/168-multiple-linear-regression-in-r/

Data Source: https://www.kaggle.com/kumarajarshi/life-expectancy-who

https://towardsdatascience.com/how-different-factors-have-an-influence-on-your-life-expectancy-7b807b04f33e
